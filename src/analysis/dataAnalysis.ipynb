{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14983191",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ece97d8",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4997003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89eb8f5",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d5c1c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "account_data = pd.read_csv(\"../../assets/clean/account.csv\")\n",
    "card_data = pd.read_csv(\"../../assets/clean/card_dev.csv\")\n",
    "client_data = pd.read_csv(\"../../assets/clean/client.csv\")\n",
    "disp_data = pd.read_csv(\"../../assets/clean/disp.csv\", dtype={\"disp_id\": int, \"client_id\": int, \"account_id\": int, \"type\": str})\n",
    "district_data = pd.read_csv(\"../../assets/clean/district.csv\")\n",
    "loan_data = pd.read_csv(\"../../assets/clean/loan_dev.csv\")\n",
    "transaction_data = pd.read_csv(\"../../assets/clean/trans_dev.csv\", dtype=\n",
    "    {\"trans_id\": int, \"account_id\": int, \"date\": str, \"type\": str, \"operation\": str, \"amount\": float, \"balance\": float, \"k_symbol\": str, \"bank\": str, \"account\": str}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb404c6",
   "metadata": {},
   "source": [
    "## Data Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b805288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable used to avoid re-running certain cells\n",
    "DATA_IS_LOADED = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4485fef5",
   "metadata": {},
   "source": [
    "### Change Pandas display settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "667d7b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "# pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddd9a62",
   "metadata": {},
   "source": [
    "### Analyze the Size of each Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72d5df83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================\n",
      "\n",
      "Number of rows for each dataset: \n",
      "\n",
      "Account: 4500\n",
      "Card: 177\n",
      "Client: 5369\n",
      "Disposition: 5369\n",
      "District: 77\n",
      "Loan: 328\n",
      "Transaction: 396685\n"
     ]
    }
   ],
   "source": [
    "print(\"=============================================\\n\")\n",
    "print(\"Number of rows for each dataset: \\n\")\n",
    "print(f\"Account: {len(account_data)}\")\n",
    "print(f\"Card: {len(card_data)}\")\n",
    "print(f\"Client: {len(client_data)}\")\n",
    "print(f\"Disposition: {len(disp_data)}\")\n",
    "print(f\"District: {len(district_data)}\")\n",
    "print(f\"Loan: {len(loan_data)}\")\n",
    "print(f\"Transaction: {len(transaction_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49c776c",
   "metadata": {},
   "source": [
    "### Get Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7698bf2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================\n",
      "\n",
      "Missing values for each dataset: \n",
      "\n",
      "Account: \n",
      "account_id     0\n",
      "district_id    0\n",
      "frequency      0\n",
      "date           0\n",
      "dtype: int64 \n",
      "\n",
      "Card: \n",
      "card_id    0\n",
      "disp_id    0\n",
      "type       0\n",
      "issued     0\n",
      "dtype: int64 \n",
      "\n",
      "Client: \n",
      "client_id       0\n",
      "birth_number    0\n",
      "district_id     0\n",
      "gender          0\n",
      "dtype: int64 \n",
      "\n",
      "Disposition: \n",
      "disp_id       0\n",
      "client_id     0\n",
      "account_id    0\n",
      "type          0\n",
      "dtype: int64 \n",
      "\n",
      "District: \n",
      "code                 0\n",
      "name                 0\n",
      "region               0\n",
      "num_inhabitants      0\n",
      "municip499           0\n",
      "municip500_1999      0\n",
      "municip2000_9999     0\n",
      "municip10000         0\n",
      "no. of cities        0\n",
      "urban_ratio          0\n",
      "avg_salary           0\n",
      "unemp_rate95         1\n",
      "unemp_rate96         0\n",
      "num_entrepreneurs    0\n",
      "num_crimes95         1\n",
      "num_crimes96         0\n",
      "dtype: int64 \n",
      "\n",
      "Loan: \n",
      "loan_id       0\n",
      "account_id    0\n",
      "date          0\n",
      "amount        0\n",
      "duration      0\n",
      "payments      0\n",
      "paid          0\n",
      "dtype: int64 \n",
      "\n",
      "Transaction: \n",
      "trans_id           0\n",
      "account_id         0\n",
      "date               0\n",
      "type               0\n",
      "operation      70761\n",
      "amount             0\n",
      "balance            0\n",
      "k_symbol      185244\n",
      "bank          299443\n",
      "account       294456\n",
      "dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=============================================\\n\")\n",
    "print(\"Missing values for each dataset: \\n\")\n",
    "print(f\"Account: \\n{account_data.isnull().sum()} \\n\")\n",
    "print(f\"Card: \\n{card_data.isnull().sum()} \\n\")\n",
    "print(f\"Client: \\n{client_data.isnull().sum()} \\n\")\n",
    "print(f\"Disposition: \\n{disp_data.isnull().sum()} \\n\")\n",
    "print(f\"District: \\n{district_data.isnull().sum()} \\n\")\n",
    "print(f\"Loan: \\n{loan_data.isnull().sum()} \\n\")\n",
    "print(f\"Transaction: \\n{transaction_data.isnull().sum()} \\n\")\n",
    "\n",
    "# TODO: Confirm if missing values are all being tracked. Values such as '?' exist in the original dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff54fa9",
   "metadata": {},
   "source": [
    "#### Replace Missing Values\n",
    "\n",
    "To have a consistent dataset, the columns containing missing values should be handled. In the `transaction` table, the `k_symbol`, `bank`, `account` and `operation` columns have missing values. The first three of those are categorical columns, so the missing values are replaced below with an `unkown` value. As for the `operation` column, it's explained below that it is strongly correlated with the `type` column, so it is dropped.\n",
    "\n",
    "Regarding the `district` table, there is one row with missing values on the columns `unemp_rate95` and `num_crimes95`. In these cases, since they are ratios, we can replace them with the mean of the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40969d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code                     39.000000\n",
      "num_inhabitants      133884.896104\n",
      "municip499               48.623377\n",
      "municip500_1999          24.324675\n",
      "municip2000_9999          6.272727\n",
      "municip10000              1.727273\n",
      "no. of cities             6.259740\n",
      "urban_ratio              63.035065\n",
      "avg_salary             9031.675325\n",
      "unemp_rate95              3.119342\n",
      "unemp_rate96              3.787013\n",
      "num_entrepreneurs       116.129870\n",
      "num_crimes95           4850.315789\n",
      "num_crimes96           5030.831169\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6566/2139339797.py:1: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  district_data.fillna(district_data.mean(), inplace=True)\n",
      "/tmp/ipykernel_6566/2139339797.py:2: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  print(district_data.mean())\n"
     ]
    }
   ],
   "source": [
    "district_data.fillna(district_data.mean(), inplace=True)\n",
    "print(district_data.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13d33bf",
   "metadata": {},
   "source": [
    "### Analyze Loans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d65679",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=============================================\\n\")\n",
    "paid_loans = loan_data[loan_data[\"paid\"] == 1]\n",
    "unpaid_loans = loan_data[loan_data[\"paid\"] == 0]\n",
    "paid_loans_percentage = round((len(paid_loans)/len(loan_data)) * 100, 2)\n",
    "print(f\"{paid_loans_percentage}% of loans are paid.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b64100",
   "metadata": {},
   "source": [
    "Approx 86% of loans have been paid (positive result). This means\n",
    "means that accuracy isn't a good metric to optimize for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343d6216",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.bar([0, 1], loan_data[\"paid\"].value_counts(), tick_label=[\"Paid\", \"Unpaid\"])\n",
    "# plt.ylim(0, 300)\n",
    "plt.title(\"Loans paid vs unpaid\")\n",
    "plt.xlabel(\"Paid\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f4ef9d",
   "metadata": {},
   "source": [
    "### Investigate attributes with a large quantity of nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567f4adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOOD: SPLIT THIS INTO DIFFERENT CELLS\n",
    "\n",
    "def parse_k_symbol(k_symb):\n",
    "    if isinstance(k_symb, float):\n",
    "        return 'none'\n",
    "    elif k_symb == \" \":\n",
    "        return 'none'\n",
    "    else:\n",
    "        return k_symb\n",
    "\n",
    "print(\"=============================================\\n\")\n",
    "print(\"[Operation]:\")\n",
    "print(transaction_data[\"operation\"].value_counts())\n",
    "operationNullsPercentage = round(len(transaction_data[transaction_data[\"operation\"].isnull()]) / len(transaction_data[\"operation\"]) * 100, 2)\n",
    "print(f\"Nulls (%): {operationNullsPercentage}%\")\n",
    "''' The operation attribute is categorical and doesn't reveal an inherited order. It can be encoded with 3 attributes\n",
    "using binary encoding. We will fill the 'nulls' with \"unknown\"\n",
    "'''\n",
    "\n",
    "transaction_data['operation'].fillna(\"unknown\", inplace=True)\n",
    "# print(transaction_data[\"operation\"].value_counts())\n",
    "\n",
    "print(\"\\n[k_symbol]:\")\n",
    "# print(transaction_data[\"k_symbol\"].value_counts())\n",
    "''' The k_symbol attribute is categorical and doesn't reveal an inherited order. It can be encoded with 3 attributes\n",
    "using binary encoding. We will replace the \" \" with \"none\"\n",
    "'''\n",
    "\n",
    "transaction_data[\"k_symbol\"] = transaction_data[\"k_symbol\"].apply(parse_k_symbol)\n",
    "print(transaction_data[\"k_symbol\"].value_counts())\n",
    "k_symbol_none_percentage = round(len(transaction_data[transaction_data[\"k_symbol\"] == \"none\"]) / len(transaction_data[\"k_symbol\"]) * 100, 2)\n",
    "print(f\"Nulls (%): {k_symbol_none_percentage}%\")\n",
    "\n",
    "\n",
    "print(\"\\n[bank]:\")\n",
    "#print(transaction_data[\"bank\"].value_counts())\n",
    "print(\"Number of unknown banks:\", len(transaction_data[transaction_data['bank'].isnull()]))\n",
    "\n",
    "print(\"\\n[account]:\")\n",
    "# print(transaction_data[\"account\"].value_counts())\n",
    "print(\"Number of unknown partners:\", len(transaction_data[transaction_data['account'].isnull()]) + len(transaction_data[transaction_data['account'] == \"0\"]))\n",
    "\n",
    "rows = transaction_data[transaction_data[\"bank\"].isnull() & (~transaction_data[\"account\"].isnull()) & (transaction_data[\"account\"] != \"0\")]\n",
    "print(f\"Number of rows where bank is empty and the account isn't: {len(rows)}\")\n",
    "'''This proves that these 2 feature are highly correlated. Although these 2 attributes aren't likely to contribute anything to our prediction, we can create an\n",
    "\"unknown\" bank and set all unknown account attributes to 0.'''\n",
    "\n",
    "transaction_data[\"account\"].fillna(0, inplace=True)\n",
    "transaction_data[\"bank\"].fillna(\"unknown\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a556002c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary method to join 2 datasets\n",
    "def join(df1, df2, key1, key2, suff, t=\"inner\"):\n",
    "    return df1.merge(df2, left_on=key1, right_on=key2, how=t, suffixes=suff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e6cda0",
   "metadata": {},
   "source": [
    "## Dimensions of Data Quality\n",
    "\n",
    "**Data quality dimension 6** - *Timeliness*\n",
    "- The data in the dataset is outdated. Consequently, it does not achieve *timeliness*\n",
    "\n",
    "**Data quality dimension 1** - *Completeness*\n",
    "- Some mandatory fields were missing, hence the data cannot be considered *complete*.\n",
    "\n",
    "**Data quality dimension 5** - *Integrity*\n",
    "- Altough the objective of our work is to predict whether a loan will end successfuly, most of the accounts do not have associated loan requests - There are orphaned records. This means that the data lacks *Integrity*\n",
    "\n",
    "**Data quality dimension 2** - *Consistency*\n",
    "- A prime example of the lack of consistency is the overlapping info between the columns *type* and *operation* of the Transactions table. This info can be seen in the [*Join Transactions* section](#transactions_type_operation_comp)\n",
    "\n",
    "**Data quality dimension 3** - *Conformity*\n",
    "- Dates follow the format: \"yy-mm-dd\" in general. However, the client's birth date does not follow this format, since it adds 50 to the month part of the number. Thus, the data is not *conformant* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d3c4b3",
   "metadata": {},
   "source": [
    "### Join Data\n",
    "\n",
    "It is needed to join all datasets into a single Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ddc60e",
   "metadata": {},
   "source": [
    "#### Join Account with Disposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4d99c5",
   "metadata": {},
   "source": [
    "Let's first analyze the Disposition dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf3be54",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of clients per disposition type:\")\n",
    "print(disp_data[\"type\"].value_counts())\n",
    "\n",
    "sb.displot(disp_data, x=\"type\", hue=\"type\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7626b1",
   "metadata": {},
   "source": [
    "All Accounts have at least 1 Disposition. There are more Dispositions than Accounts, since some clients are **owners** while others are **disponent owners** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b3226b",
   "metadata": {},
   "source": [
    "We will create an attribute on the Account table that reflects whether the account is co-owned. Note that the DISPONENTs will be dropped from the joined table. The type attribute can be dropped as well since all dispositions are Owners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31214c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = join(account_data, disp_data, \"account_id\", \"account_id\", [\"\", \"_disp\"])\n",
    "df.rename(columns={\"date\": \"a_date\"}, inplace=True)\n",
    "\n",
    "# Count Groups\n",
    "owner_count = df[\"account_id\"].value_counts()\n",
    "df[\"is_co-owned\"] = df.apply(lambda row: 1 if owner_count[row[\"account_id\"]] > 1 else 0, axis='columns')\n",
    "\n",
    "# Cleanup\n",
    "df.drop(df[df[\"type\"] == \"DISPONENT\"].index, inplace=True)\n",
    "df.drop(columns=[\"type\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46211c3f",
   "metadata": {},
   "source": [
    "TODO: The column \"disp_id\" might also be useless since it's a 1-1 relation with the account now. But we\n",
    " will drop it after proving the correlation between the 2 attributes.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d8866c",
   "metadata": {},
   "source": [
    "### Join Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb731d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DATA_IS_LOADED:\n",
    "    df = join(df, client_data, \"client_id\", \"client_id\", [\"\", \"_client\"], t=\"left\")\n",
    "    df.drop(['client_id'], axis='columns', inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0bc8b8",
   "metadata": {},
   "source": [
    "#### Join Districts\n",
    "\n",
    "Both the *Account* and the *Client* have a foreign key to the *District* table\n",
    "We will for now add *District* info about both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d3ccad",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DATA_IS_LOADED:\n",
    "    # Add Client's District Data -> Change to MD\n",
    "    df = join(df, district_data, \"district_id_client\", \"code\", [\"\", \"_district\"], t=\"inner\")\n",
    "    df.drop(['district_id_client'], axis='columns', inplace=True)\n",
    "\n",
    "    # Add Accounts's District Data -> Change to MD\n",
    "    df = join(df, district_data, \"district_id\", \"code\", [\"_aDistrict\", \"_cDistrict\"], t=\"inner\")\n",
    "    df.drop(['district_id'], axis='columns', inplace=True)\n",
    "    \n",
    "# df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f946cf",
   "metadata": {},
   "source": [
    "Note that the district information regarding the *Account* contains an `a` prefix, while the *Client* info contains a `c` prefix.\n",
    "\n",
    "Some of the Columns such as: *code_aDistrict* and *name_aDistrict* seem to be ambiguous. However, we will first confirm their correlation before removing them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbea65b9",
   "metadata": {},
   "source": [
    "The next 3 tables have training and testing versions. Since we will do the data analysis of the training version, we'll ignore the other one for now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56c1bea",
   "metadata": {},
   "source": [
    "### Join Loan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8f4268",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DATA_IS_LOADED:\n",
    "    # Merge to the right, since accounts that don't have loans are not relevant\n",
    "    df = join(df, loan_data, \"account_id\", \"account_id\", [\"\", \"_loan\"], t=\"right\")\n",
    "    df.drop(['loan_id'], axis='columns', inplace=True)\n",
    "    df.rename(columns={\"date\": \"loan_date\"}, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f69100",
   "metadata": {},
   "source": [
    "### Join Card"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2036be97",
   "metadata": {},
   "source": [
    "Almost no *Client* in the dataset has a *Card*. This is rather problematic, since it creates a lot of missing values or lost of information.\n",
    "\n",
    "The *Card* dataset contains only 2 useful columns:\n",
    "- Card type -- can be ordered from worst to best\n",
    "- Issuance date\n",
    "\n",
    "The type of Card helps us rank clients. A *Client* with a gold card is better than a client with a junior card. On the other hand, the **issuance date** allows us to consider only cards that have been issued before the loan, as information after the loan is irrelevant for this prediction task. TODO: THIS LAST PART IS NOT DONE\n",
    "\n",
    "The Card **type** is a categorical field:\n",
    "- *junior* -- underage people (lowest tier), usually it isn't possible to withdraw money from them\n",
    "- *classic*\n",
    "- *gold* -- highest tier\n",
    "\n",
    "**Since the field is ordered in tiers, in the cleaning process we should convert these values to a numerical scale where:**\n",
    "- 0 -> no card\n",
    "- 1 -> junior\n",
    "- 2 -> classic\n",
    "- 3 -> gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224e027d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DATA_IS_LOADED:\n",
    "    card_disp = join(card_data, disp_data, \"disp_id\", \"disp_id\", [\"_card\", \"_disp\"])\n",
    "    card_disp = card_disp.groupby([card_disp.account_id, card_disp.type_card]).size().unstack(fill_value=0)\n",
    "card_disp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a507a55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DATA_IS_LOADED:\n",
    "    df = join(df, card_data, \"disp_id\", \"disp_id\", [\"\", \"_card\"], t=\"left\")\n",
    "    df.fillna(\"none\", inplace=True)\n",
    "    df.drop(['card_id', 'disp_id'], axis='columns', inplace=True)\n",
    "    df.rename(columns={\"type\": \"card_type\", \"issued\": \"card_issued\"}, inplace=True)\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dbf413",
   "metadata": {},
   "source": [
    "There are very few clients with cards. Every *Client* with a card has been granted a loan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4579161c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"card_type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba86180",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"card_type\"] != \"none\"][\"paid\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f742f39",
   "metadata": {},
   "source": [
    "### Join Transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c67eb6a",
   "metadata": {},
   "source": [
    "We need to solve an issue with the *Transactions* table -- There are many transactions for some accounts. So, joining them would create multiple rows for each loan.\n",
    "\n",
    "We need to find a way to extract the useful information from this table to merge it into the dataset.\n",
    "\n",
    "Note that in this table the *account_id* columns refers to the **target account** of the operation, while the *account* refers to the source "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aeb86a8",
   "metadata": {},
   "source": [
    "This table contains multiple categorical attributes. Before working with it, we should explore in more detail what they represent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a506523e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_data['type'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b65faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_data['operation'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5575245",
   "metadata": {},
   "source": [
    "<a id=\"transactions_type_operation_comp\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d30fa41",
   "metadata": {},
   "source": [
    "The fields: *type* and *operation* seem to transmit identical information, where the *operation* goes into more detail about the type of transaction. Thus, after investigating the dataset we found the following mapping:\n",
    "\n",
    "| type               | operation                                                              |\n",
    "|--------------------|------------------------------------------------------------------------|\n",
    "| credit             | unknown; collection from another bank, credit in cash                  |\n",
    "| withdrawal         | credit card withdrawal; remittance to another bank; withdrawal in cash |\n",
    "| withdrawal in cash | withdrawal in cash                                                     |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f393c559",
   "metadata": {},
   "source": [
    "Consequently, this ambiguity will be taken into account towards the building of the final dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d259f48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_data['k_symbol'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dba2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_missing_accounts():\n",
    "    account_ids = df[\"account_id\"].unique()\n",
    "    return transaction_data.copy().loc[transaction_data[\"account_id\"].isin(account_ids)]\n",
    "\n",
    "if not DATA_IS_LOADED:\n",
    "    active_transactions = filter_missing_accounts()\n",
    "\n",
    "active_transactions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f96866",
   "metadata": {},
   "source": [
    "Remove *Transactions* that happened after the respective loan decision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48b3f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_COUNT_LATE_TRANSACTIONS = False # Bolean to skip this instruction if desired\n",
    "\n",
    "def count_late_transactions():\n",
    "    counter = 0\n",
    "    for id, row in active_transactions.iterrows():\n",
    "        transaction_date = row[\"date\"]\n",
    "        acc_id = row[\"account_id\"]\n",
    "        for _, loan_row in df[df[\"account_id\"] == acc_id].iterrows():\n",
    "            loan_date = loan_row[\"loan_date\"]\n",
    "            if transaction_date > loan_date:\n",
    "                counter += 1\n",
    "    \n",
    "    return counter\n",
    "\n",
    "if RUN_COUNT_LATE_TRANSACTIONS:\n",
    "    if not DATA_IS_LOADED:\n",
    "        late_transactions_count = count_late_transactions()\n",
    "\n",
    "    print(f\"Number of late transactions: {late_transactions_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1ec77b",
   "metadata": {},
   "source": [
    "All *Transactions* happened before the loan decision, so none were dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cd8ca1",
   "metadata": {},
   "source": [
    "To simplify the understanding of transactions:\n",
    "- Withdrawals will have a negative *balance*\n",
    "- Credit will have a positive *balance*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c95c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_balance(transactions):\n",
    "    def convert_bal_inner(row):\n",
    "        amount = abs(row['amount'])\n",
    "        return amount if row['type'] == 'credit' else -amount\n",
    "    \n",
    "    transactions['amount'] = transactions.apply(convert_bal_inner, axis='columns')\n",
    "    return transactions\n",
    "\n",
    "if not DATA_IS_LOADED:\n",
    "    active_transactions = convert_balance(active_transactions)\n",
    "active_transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7362d651",
   "metadata": {},
   "source": [
    "Regarding the *bank* and *account* columns, most entries are empty (83%). So these columns were considered not useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91da71a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_transactions[active_transactions['bank'] == \"unknown\"][\"bank\"].count() / active_transactions[\"bank\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454248e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DATA_IS_LOADED:\n",
    "    active_transactions.drop(['bank', 'account'], axis='columns', inplace=True)\n",
    "\n",
    "active_transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684a2e74",
   "metadata": {},
   "source": [
    "### Sanctions for Negative Balance\n",
    "Knowing the number of times each *User* was sanctioned for **negative balance** might be useful to know which loans to reject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a103d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_sanctions(df, active_transactions):\n",
    "    grouped = active_transactions.groupby('account_id')\n",
    "\n",
    "    sanction_df = pd.DataFrame(columns=['account_id', 'sanctions'])\n",
    "    for acc_id, group in grouped:\n",
    "        sanction_count = group[group['k_symbol'] == \"sanction interest if negative balance\"][\"k_symbol\"].count()\n",
    "        sanction_df = pd.concat([sanction_df, pd.DataFrame({'account_id': acc_id, 'sanctions': sanction_count}, index=[0])], ignore_index=True)\n",
    "\n",
    "    df[\"sanctions\"] = sanction_df[\"sanctions\"]\n",
    "    df[\"sanctions\"].fillna(0.0, inplace=True)\n",
    "    return df\n",
    "\n",
    "if not DATA_IS_LOADED:\n",
    "    df = count_sanctions(df, active_transactions)\n",
    "\n",
    "# df[\"sanctions\"].value_counts()\n",
    "df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f96a61",
   "metadata": {},
   "source": [
    "### Mean Household Payments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f0ce5e",
   "metadata": {},
   "source": [
    "Knowing how much a client pays for its house may be a useful metric to know its financial situation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27cffed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def household_mean(df, active_transactions):\n",
    "    grouped = active_transactions.groupby(\"account_id\")\n",
    "    household_df = pd.DataFrame(columns=[\"account_id\", \"household_mean\"])\n",
    "\n",
    "    for acc_id, group in grouped:\n",
    "        house_mean = abs(group[group[\"k_symbol\"] == \"household\"][\"amount\"].mean())\n",
    "        household_df = pd.concat([household_df, pd.DataFrame({'account_id': acc_id, 'household_mean': house_mean}, index=[0])], ignore_index=True)\n",
    "\n",
    "    df[\"household_mean\"] = household_df[\"household_mean\"]\n",
    "    df[\"household_mean\"].fillna(0.0, inplace=True)\n",
    "    return df\n",
    "\n",
    "if not DATA_IS_LOADED:\n",
    "    df = household_mean(df, active_transactions)\n",
    "\n",
    "df[\"household_mean\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d19375",
   "metadata": {},
   "source": [
    "### Mean Interest Payments\n",
    "Knowing how much a client earns in **account interest** might be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cda3363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interest_mean(df, active_transactions):\n",
    "    grouped = active_transactions.groupby(\"account_id\")\n",
    "    interest_df = pd.DataFrame(columns=[\"account_id\", \"interest_mean\"])\n",
    "\n",
    "    for acc_id, group in grouped:\n",
    "        interest_mean = abs(group[group[\"k_symbol\"] == \"interest credited\"][\"amount\"].mean())\n",
    "        interest_df = pd.concat([interest_df, pd.DataFrame({'account_id': acc_id, 'interest_mean': interest_mean}, index=[0])], ignore_index=True)\n",
    "\n",
    "    df[\"interest_mean\"] = interest_df[\"interest_mean\"]\n",
    "    df[\"interest_mean\"].fillna(0.0, inplace=True)\n",
    "    return df\n",
    "\n",
    "if not DATA_IS_LOADED:\n",
    "    df = interest_mean(df, active_transactions)\n",
    "\n",
    "df[\"interest_mean\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1c2b1f",
   "metadata": {},
   "source": [
    "### Mean Balance in Time References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0068bb9b",
   "metadata": {},
   "source": [
    "The mean balance in the account **1 month**, **1 semester**, and **1 year** before the loan request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6d31cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_time_references(df, active_transactions):\n",
    "    def to_datetime(df, col_name=\"date\"):\n",
    "        df[col_name + \"_dt\"] = pd.to_datetime(df[col_name], format=\"%Y-%m-%d\")\n",
    "    \n",
    "    to_datetime(df, \"loan_date\")\n",
    "    to_datetime(active_transactions)\n",
    "\n",
    "    # Join current dataset with transactions so that we can group by account_id\n",
    "    temp_df = join(df, active_transactions, \"account_id\", \"account_id\", suff=[\"\", \"_trans\"])[[\"account_id\", \"trans_id\", \"date_dt\", \"balance\", \"loan_date_dt\"]]\n",
    "\n",
    "    # Calculate month's offset between each transaction and loan date\n",
    "    temp_df['month_diff'] = (temp_df['loan_date_dt'] - temp_df['date_dt']) / np.timedelta64(1, 'M')\n",
    "\n",
    "    # \n",
    "    def avg_balance(dataframe, balance_name):\n",
    "        return dataframe.groupby('account_id').mean()['balance'].reset_index().rename(columns={'balance': balance_name})\n",
    "\n",
    "    # Create 3 Tables for the average balance in an account in the previous month, semester and year (account_id, balance)\n",
    "    prev_year = avg_balance(temp_df.loc[temp_df['month_diff'] <= 12], 'prev_year_balance') \n",
    "    prev_semester = avg_balance(temp_df.loc[temp_df['month_diff'] <= 6], 'prev_6m_balance')\n",
    "    prev_month = avg_balance(temp_df.loc[temp_df['month_diff'] <= 1], 'prev_1m_balance')\n",
    "\n",
    "    mergedDf = df.merge(prev_year, on='account_id', how='inner')\n",
    "    mergedDf = mergedDf.merge(prev_semester, on='account_id', how='inner')\n",
    "    mergedDf = mergedDf.merge(prev_month, on='account_id', how='inner')\n",
    "\n",
    "    mergedDf.drop(['loan_date_dt'], axis='columns', inplace=True)\n",
    "\n",
    "    return mergedDf\n",
    "\n",
    "if not DATA_IS_LOADED:\n",
    "    df = balance_time_references(df, active_transactions)\n",
    "\n",
    "df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad6c6fc",
   "metadata": {},
   "source": [
    "### Min, Max and Mean Balances\n",
    "\n",
    "it's also relevant to store some statistics regarding the balance of the clients along time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0cbd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def client_balance(df, active_transactions):\n",
    "    grouped = active_transactions.groupby(\"account_id\")\n",
    "    balance_df = pd.DataFrame(columns=[\"account_id\", \"balance_min\", \"balance_max\", \"balance_mean\"])\n",
    "\n",
    "    for acc_id, group in grouped:\n",
    "        client_balance = group[\"balance\"]\n",
    "        balance_df = pd.concat([balance_df, pd.DataFrame({'account_id': acc_id, 'balance_min': client_balance.min(), 'balance_max': client_balance.max(), \n",
    "            'balance_mean': client_balance.mean()}, index=[0])], ignore_index=True)\n",
    "\n",
    "    balance_df.fillna(0.0, inplace=True)\n",
    "    df = join(df, balance_df, \"account_id\", \"account_id\", suff=['', ''])\n",
    "    return df\n",
    "\n",
    "if not DATA_IS_LOADED:\n",
    "    df = client_balance(df, active_transactions)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471f31a4",
   "metadata": {},
   "source": [
    "### Average balance Influx\n",
    "Knowing the **balance variation** of a client might prove to be an useful metric, as it portrays the tendencies of the account. Consequently, we will create 3 columns to store the average influx (money IN - money OUT) in the last 3, 6 and 12 months.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347be73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def client_bal_influx(df, active_transactions):\n",
    "    def to_datetime(df, col_name=\"date\"):\n",
    "        df[col_name + \"_dt\"] = pd.to_datetime(df[col_name], format=\"%Y-%m-%d\")\n",
    "\n",
    "    to_datetime(df, \"loan_date\")\n",
    "    to_datetime(active_transactions)\n",
    "\n",
    "    # Join current dataset with transactions so that we can group by account_id\n",
    "    temp_df = join(df, active_transactions, \"account_id\", \"account_id\", suff=[\"\", \"_trans\"])[[\"account_id\", \"date_dt\", \"loan_date_dt\", \"amount_trans\"]]\n",
    "    temp_df.rename(columns={'amount_trans': 'influx'}, inplace=True)\n",
    "\n",
    "    # Calculate month's offset between each transaction and loan date\n",
    "    temp_df['month_diff'] = (temp_df['loan_date_dt'] - temp_df['date_dt']) / np.timedelta64(1, 'M')\n",
    "\n",
    "    # Calculate sum of influxes from the given Rows\n",
    "    def balance_influx(dataframe, balance_name):\n",
    "        return dataframe.groupby('account_id').sum(numeric_only=True)['influx'].reset_index().rename(columns={'influx': balance_name})\n",
    "\n",
    "    # Create 3 Tables for the average influx in the last 3, 6 and 12 months\n",
    "    prev_year = balance_influx(temp_df.loc[temp_df['month_diff'] <= 12], 'mean_year_influx')\n",
    "    prev_year['mean_year_influx'] = prev_year['mean_year_influx'] / 12  # Average Influx per Month\n",
    "\n",
    "    prev_6month = balance_influx(temp_df.loc[temp_df['month_diff'] <= 6], 'mean_6m_influx')\n",
    "    prev_6month['mean_6m_influx'] = prev_6month['mean_6m_influx'] / 6  # Average Influx per Month\n",
    "\n",
    "    prev_3month = balance_influx(temp_df.loc[temp_df['month_diff'] <= 3], 'mean_3m_influx')\n",
    "    prev_3month['mean_3m_influx'] = prev_3month['mean_3m_influx'] / 3  # Average Influx per Month\n",
    "\n",
    "    mergedDf = df.merge(prev_year, on='account_id', how='inner')\n",
    "    mergedDf = mergedDf.merge(prev_6month, on='account_id', how='inner')\n",
    "    mergedDf = mergedDf.merge(prev_3month, on='account_id', how='inner')\n",
    "\n",
    "    mergedDf.drop(['loan_date_dt'], axis='columns', inplace=True)\n",
    "\n",
    "    return mergedDf\n",
    "\n",
    "if not DATA_IS_LOADED:\n",
    "    df = client_bal_influx(df, active_transactions)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9aad720",
   "metadata": {},
   "source": [
    "Now that we have extracted the desired information from all the tables, we can drop the account_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8da93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DATA_IS_LOADED:\n",
    "     df.drop(['account_id'], axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b638ba",
   "metadata": {},
   "source": [
    "### Check Duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90dca18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df.drop_duplicates().index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668424a1",
   "metadata": {},
   "source": [
    "There are no duplicate rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1057b2f",
   "metadata": {},
   "source": [
    "## Save the Joined Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211f9e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "filepath = Path(\"../../assets/joined\")\n",
    "filepath.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df.to_csv(\"../../assets/joined/data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f766a71",
   "metadata": {},
   "source": [
    "### Final Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878a864d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a9307a",
   "metadata": {},
   "source": [
    "==================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab81c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable used to avoid re-running certain cells\n",
    "DATA_IS_LOADED = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9e65b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
